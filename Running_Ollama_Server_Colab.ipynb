{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHMI6ZBWOq4M"
      },
      "source": [
        "**Execute this code in Google Colab and establish a ngrok tunnel to connect with the Ollama server hosted on Colab's T4 GPU. This approach is designed for evaluation purposes, providing faster inference for systems with very limited computational resources.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mN5hLpAKCrN"
      },
      "source": [
        "1. ***Check GPU and Install Ollama***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9WCW10rJrZA"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0AF53FKIbs"
      },
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "!nvidia-smi\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ GPU detected! Ready to run Ollama\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8AMoWLcKJkj"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Ollama\n",
        "print(\"üì¶ Installing Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\n‚úÖ Ollama installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdGePSGfKT4E"
      },
      "source": [
        "2. **Start Ollama server in Background**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pFV8cLmKXeC"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üöÄ Starting Ollama server...\")\n",
        "ollama_process = subprocess.Popen(\n",
        "    ['ollama', 'serve'],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# Check if running\n",
        "!curl -s http://localhost:11434/api/tags > /dev/null && echo \"‚úÖ Ollama server running on port 11434\" || echo \"‚ö†Ô∏è  Server not responding\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L33EDEyvKe8T"
      },
      "outputs": [],
      "source": [
        "# Stop any old Ollama server\n",
        "!pkill ollama\n",
        "\n",
        "# Start Ollama bound to all interfaces + allow any origins\n",
        "!OLLAMA_HOST=0.0.0.0 OLLAMA_ORIGINS=* ollama serve > /tmp/ollama.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO57LsfDKhf_"
      },
      "outputs": [],
      "source": [
        "!lsof -i:11434"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOwue71QKoUB"
      },
      "outputs": [],
      "source": [
        "# check running models in colab\n",
        "import requests\n",
        "r = requests.get(\"http://localhost:11434/api/tags\")\n",
        "print(r.status_code)\n",
        "print(r.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CucT4emaKvCW"
      },
      "source": [
        "3. **Pull Models and Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcA4Xh0b7gQA"
      },
      "outputs": [],
      "source": [
        "!ollama pull llama3.1:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYx99Fbz7f07"
      },
      "outputs": [],
      "source": [
        "!ollama pull mistral:7b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYdyDjJSYS6b"
      },
      "outputs": [],
      "source": [
        "# check running models in colab\n",
        "import requests\n",
        "r = requests.get(\"http://localhost:11434/api/tags\")\n",
        "print(r.status_code)\n",
        "print(r.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JuWqzhqbneO"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "\n",
        "# Test inference\n",
        "test_prompt = \"\"\"You are an expert SQL developer.\n",
        "\n",
        "### Database Schema:\n",
        "CREATE TABLE customers (\n",
        "    CustomerID INTEGER PRIMARY KEY,\n",
        "    Segment TEXT,\n",
        "    Revenue REAL\n",
        ");\n",
        "\n",
        "### Task:\n",
        "Generate SQL to answer: \"What is the total revenue by customer segment?\"\n",
        "\n",
        "```sql\n",
        "\"\"\"\n",
        "\n",
        "print(\"üß™ Testing model inference...\\n\")\n",
        "print(\"=\"*80)\n",
        "!echo '{test_prompt}' | ollama run {MODEL_NAME}\n",
        "print(\"=\"*80)\n",
        "print(\"\\n‚úÖ Model inference working!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8GMzAEtLwOa"
      },
      "source": [
        "## 6. Setup ngrok Tunnel\n",
        "\n",
        "**This exposes your Ollama server publicly so your local CESMA agent can connect to it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTvH6YLRLy_3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"http://localhost:11434/api/generate\"\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "payload = {\n",
        "    \"model\": \"llama3.1:8b\",\n",
        "    \"prompt\": \"Write a simple SQL query to list all customers from a table named customers\"\n",
        "}\n",
        "\n",
        "resp = requests.post(url, headers=headers, json=payload, stream=True)\n",
        "\n",
        "for line in resp.iter_lines():\n",
        "    if line:\n",
        "        print(line.decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg0sIjBvXMgb"
      },
      "outputs": [],
      "source": [
        "# Install pyngrok\n",
        "!pip install -q pyngrok\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import requests\n",
        "\n",
        "# Set ngrok auth token\n",
        "print(\"üîë Enter your ngrok auth token\")\n",
        "print(\"Get it from: https://dashboard.ngrok.com/get-started/your-authtoken\\n\")\n",
        "NGROK_TOKEN = input(\"ngrok token: \").strip()\n",
        "\n",
        "ngrok.set_auth_token(NGROK_TOKEN)\n",
        "print(\"‚úÖ ngrok token set\")\n",
        "\n",
        "# Optional: Configure ngrok for paid tier (removes browser warning)\n",
        "print(\"\\nüíé Paid tier detected - configuring advanced settings...\")\n",
        "print(\"This will bypass the browser warning page for API requests.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwyGWw68bneP"
      },
      "outputs": [],
      "source": [
        "# Create tunnel with paid tier configuration\n",
        "print(\"üåê Creating ngrok tunnel to Ollama server...\\n\")\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Configure ngrok options for paid tier\n",
        "# Paid tier automatically removes the browser warning page\n",
        "ngrok_options = {\n",
        "    \"bind_tls\": True,  # HTTPS only\n",
        "}\n",
        "\n",
        "# For paid tier, you can also add:\n",
        "# - Custom subdomain: \"subdomain\": \"your-custom-name\"\n",
        "# - IP restrictions: \"ip_restriction\": {\"allow_cidrs\": [\"0.0.0.0/0\"]}\n",
        "# - OAuth: \"oauth\": {\"provider\": \"google\", \"allow_emails\": [\"you@example.com\"]}\n",
        "\n",
        "# Ask if user wants custom subdomain (paid tier feature)\n",
        "use_custom_subdomain = input(\n",
        "    \"\\nüîπ Use custom subdomain? (paid tier only, y/n): \").strip().lower()\n",
        "if use_custom_subdomain == 'y':\n",
        "    custom_subdomain = input(\n",
        "        \"Enter subdomain name (e.g., 'my-cesma'): \").strip()\n",
        "    if custom_subdomain:\n",
        "        ngrok_options[\"subdomain\"] = custom_subdomain\n",
        "        print(f\"‚úÖ Will use custom subdomain: {custom_subdomain}.ngrok.io\")\n",
        "\n",
        "# Create new tunnel on port 11434 (Ollama default)\n",
        "tunnel = ngrok.connect(11434, **ngrok_options)\n",
        "\n",
        "# Extract the public URL as a string\n",
        "public_url = str(tunnel.public_url)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ ngrok tunnel created!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüåê Public URL: {public_url}\")\n",
        "print(f\"\\nüíé Paid tier benefits:\")\n",
        "print(\"   ‚úÖ No browser warning page (403 errors fixed)\")\n",
        "print(\"   ‚úÖ Higher bandwidth limits\")\n",
        "print(\"   ‚úÖ Custom subdomain (if configured)\")\n",
        "print(\"   ‚úÖ Better reliability\")\n",
        "\n",
        "print(f\"\\nüìù Use this URL in your CESMA config.yaml:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\"\"ollama:\n",
        "  base_url: \"{public_url}\"\n",
        "  sql_generator_model: \"{MODEL_NAME}\"\n",
        "  query_decomposer_model: \"{MODEL_NAME}\"\n",
        "  temperature: 0\"\"\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test tunnel with ngrok bypass header\n",
        "print(\"\\nüß™ Testing tunnel connection...\")\n",
        "\n",
        "# Headers to bypass ngrok browser warning (works on free and paid tier)\n",
        "headers = {\n",
        "    'ngrok-skip-browser-warning': 'true',\n",
        "    'User-Agent': 'CESMA-SQL-Agent/1.0'\n",
        "}\n",
        "\n",
        "try:\n",
        "    # Test with headers\n",
        "    response = requests.get(f\"{public_url}/api/tags\",\n",
        "                            headers=headers, timeout=10)\n",
        "    if response.status_code == 200:\n",
        "        print(\"‚úÖ Tunnel is working perfectly!\")\n",
        "        print(f\"üìã Available models: {response.json()}\")\n",
        "        print(\"\\n‚ö†Ô∏è  NOTE: Still seeing 403? The issue is ngrok still shows warning page.\")\n",
        "        print(\"   Solution: Visit the URL in browser once, then API calls will work.\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Status: {response.status_code}\")\n",
        "        print(f\"Response: {response.text[:200]}\")\n",
        "\n",
        "        # Try to diagnose the issue\n",
        "        print(\"\\nüîç Diagnosing issue...\")\n",
        "        print(f\"1. Visit this URL in your browser: {public_url}\")\n",
        "        print(f\"2. Click 'Visit Site' if you see ngrok warning page\")\n",
        "        print(f\"3. You should see Ollama API response\")\n",
        "        print(f\"4. Then re-run this cell to verify connection\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection error: {e}\")\n",
        "    print(\"\\nüîß Troubleshooting:\")\n",
        "    print(\"1. Check if Ollama server is running locally:\")\n",
        "    print(\"   Run: curl http://localhost:11434/api/tags\")\n",
        "    print(\"2. Verify ngrok token and account type\")\n",
        "    print(\"3. Check ngrok dashboard: https://dashboard.ngrok.com/tunnels\")\n",
        "    print(f\"4. Visit {public_url} in browser to activate tunnel\")\n",
        "\n",
        "# Additional check - test local Ollama server\n",
        "print(\"\\nüîç Checking local Ollama server...\")\n",
        "try:\n",
        "    local_response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "    if local_response.status_code == 200:\n",
        "        print(\"‚úÖ Local Ollama server is running correctly\")\n",
        "        print(\n",
        "            f\"   Models: {[m['name'] for m in local_response.json().get('models', [])]}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Local server issue: {local_response.status_code}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Local Ollama not responding: {e}\")\n",
        "    print(\"   Please check if Ollama is running (see cell above)\")\n",
        "\n",
        "# Save URL for later use\n",
        "print(f\"\\nüíæ Your ngrok URL: {public_url}\")\n",
        "print(\"Keep this notebook running to maintain the tunnel!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìå IMPORTANT: To fix 403 errors from your local machine:\")\n",
        "print(\"=\"*80)\n",
        "print(\"1. Open browser and visit: \" + public_url)\n",
        "print(\"2. Click 'Visit Site' on ngrok warning page (if shown)\")\n",
        "print(\"3. After that, API calls from CESMA will work\")\n",
        "print(\"\\nAlternatively, ngrok paid tier should have a setting to disable\")\n",
        "print(\"the warning page completely. Check: https://dashboard.ngrok.com/settings\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orITL4gjMKMo"
      },
      "source": [
        "## 7. Keep-Alive Script\n",
        "\n",
        "**Run this cell to keep the session alive (prevents Colab from disconnecting)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p49r_7-aMNvF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datetime import datetime\n",
        "import requests\n",
        "\n",
        "print(\"üîÑ Keep-alive script started\")\n",
        "print(\"This will ping the server every 5 minutes to prevent timeout\\n\")\n",
        "print(\"Press Ctrl+C or interrupt the cell to stop\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ping_count = 0\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        ping_count += 1\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        # Ping local Ollama server\n",
        "        try:\n",
        "            response = requests.get(\n",
        "                \"http://localhost:11434/api/tags\", timeout=5)\n",
        "            status = \"‚úÖ OK\" if response.status_code == 200 else f\"‚ö†Ô∏è  {response.status_code}\"\n",
        "        except Exception as e:\n",
        "            status = f\"‚ùå Error: {str(e)[:50]}\"\n",
        "\n",
        "        print(f\"[{timestamp}] Ping #{ping_count}: {status}\")\n",
        "\n",
        "        # Wait 5 minutes\n",
        "        time.sleep(300)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\\nüõë Keep-alive stopped\")\n",
        "    print(f\"Total pings: {ping_count}\")\n",
        "    print(f\"Uptime: ~{ping_count * 5} minutes\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
